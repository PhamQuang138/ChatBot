#!/usr/bin/env python3
import os
import re
import json
import torch
import numpy as np
import gradio as gr
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModel,
    BitsAndBytesConfig,
    pipeline
)
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate
from rank_bm25 import BM25Okapi


# ================== CONFIG ==================
BASE_DIR = "/home/quang/Documents/ChatBot"
CHROMA_PATH = os.path.join(BASE_DIR, "data", "chroma_db_qwen_embed_vn")
LLM_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"        # ‚úÖ ƒê√£ thay 3B
EMBED_MODEL = "Qwen/Qwen3-Embedding-0.6B"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOP_K = 20
THRESHOLD = 0.005
BATCH_SIZE = 4
# ===========================================


# ===== Embedding wrapper =====
class Qwen3Embedding(Embeddings):
    def __init__(self, model, tokenizer, device="cpu", batch_size=4):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.batch_size = batch_size

    def embed_documents(self, texts):
        all_embs = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True,
                                    truncation=True, max_length=512).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
                vecs = outputs.last_hidden_state[:, 0, :]
                vecs = vecs / (vecs.norm(dim=-1, keepdim=True) + 1e-12)
                all_embs.append(vecs.cpu().numpy())
        if not all_embs:
            return np.zeros((0, self.model.config.hidden_size)).tolist()
        return np.vstack(all_embs).tolist()

    def embed_query(self, text):
        return self.embed_documents([text])[0]


# ===== Utility =====
def cosine_similarity(a, b):
    a, b = np.asarray(a), np.asarray(b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    return float(np.dot(a, b) / denom) if denom != 0 else 0.0


def embed_query_vector(text: str, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)
        emb = outputs.last_hidden_state[:, 0, :]
        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-12)
    return emb.cpu().numpy()[0]


# ===== Initialization =====
vectordb = None
retriever = None
llm = None
prompt_template = None
embed_model = None
embed_tokenizer = None
embedding_fn = None

def initialize_rag_components():
    global vectordb, retriever, llm, prompt_template, embed_model, embed_tokenizer, embedding_fn

    print("üõ†Ô∏è Initializing RAG components...")

    # 1Ô∏è‚É£ Load embedding model
    print(f"üîπ Loading embedding model: {EMBED_MODEL}")
    embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)
    embed_model = AutoModel.from_pretrained(
        EMBED_MODEL,
        device_map="auto",
        torch_dtype=torch.float16
    )
    try:
        embed_model.to(DEVICE)
    except Exception:
        pass
    embedding_fn = Qwen3Embedding(embed_model, embed_tokenizer, DEVICE, BATCH_SIZE)
    print("‚úÖ Embedding model ready.")

    # 2Ô∏è‚É£ Load Chroma DB
    if not os.path.exists(CHROMA_PATH):
        raise FileNotFoundError(f"‚ùå Chroma DB not found at {CHROMA_PATH}")
    print(f"üîπ Loading Chroma DB from {CHROMA_PATH}")
    vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_fn)
    retriever = vectordb.as_retriever(search_kwargs={"k": TOP_K})
    print("‚úÖ Chroma retriever ready.")

    # 3Ô∏è‚É£ Load LLM
    print(f"üîπ Loading LLM {LLM_MODEL} (4-bit)...")
    bnb_config = BitsAndBytesConfig(load_in_4bit=True)

    tokenizer_llm = AutoTokenizer.from_pretrained(LLM_MODEL)
    model_llm = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        quantization_config=bnb_config,
        low_cpu_mem_usage=True
    )

    # ‚ö° Load LoRA adapter (n·∫øu c√≥)
    lora_path = os.path.join(BASE_DIR, "src", "lora_qwen_druglaw_4bit")
    if os.path.exists(lora_path):
        try:
            from peft import PeftModel
            print(f"üîπ Attaching LoRA adapter from {lora_path}")
            model_llm = PeftModel.from_pretrained(model_llm, lora_path)
            print("‚úÖ LoRA adapter loaded successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Kh√¥ng th·ªÉ load LoRA adapter: {e}")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c LoRA t·∫°i {lora_path}")

    llm = pipeline(
        "text-generation",
        model=model_llm,
        tokenizer=tokenizer_llm,
        return_full_text=False
    )

    print("‚úÖ LLM ready.")

prompt_template_normal = ChatPromptTemplate.from_template(
        """B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ **Lu·∫≠t D∆∞·ª£c Vi·ªát Nam**.

    D·ª±a **ch·ªâ tr√™n ph·∫ßn CONTEXT d∆∞·ªõi ƒë√¢y**, h√£y **tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh ph√°p lu·∫≠t** c√≥ li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
    Kh√¥ng ƒë∆∞·ª£c:
    - th√™m b√¨nh lu·∫≠n, suy lu·∫≠n, hay di·ªÖn gi·∫£i.
    - li·ªát k√™ c√°c l·ª±a ch·ªçn ki·ªÉu a), b), c) n·∫øu c√¢u h·ªèi kh√¥ng y√™u c·∫ßu.
    - t·ª± ƒë√°nh gi√° hay ch·ªçn ƒë√°p √°n.

    ---
    üìò CONTEXT:
    {context}

    üí¨ C√ÇU H·ªéI:
    {question}

    ‚úçÔ∏è TR·∫¢ L·ªúI (tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh):
    """
    )

prompt_template_quiz = ChatPromptTemplate.from_template(
        """B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ **Lu·∫≠t D∆∞·ª£c Vi·ªát Nam**.

    C√¢u h·ªèi sau ƒë√¢y c√≥ d·∫°ng **tr·∫Øc nghi·ªám nhi·ªÅu l·ª±a ch·ªçn** (a, b, c, d...).
    "Ch·ªâ tr·∫£ l·ªùi c√°c m·ª•c a) t·ªõi h) ƒë√£ cho, KH√îNG sinh th√™m nh√£n hay A:, B:, C: tr·ªëng."
    D·ª±a **ch·ªâ tr√™n ph·∫ßn CONTEXT**, h√£y:
    - tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh li√™n quan, 
    - sau ƒë√≥ **ch·ªâ ra ƒë√°p √°n ƒë√∫ng duy nh·∫•t**, kh√¥ng th√™m gi·∫£i th√≠ch hay b√¨nh lu·∫≠n.

    ---
    üìò CONTEXT:
    {context}

    üí¨ C√ÇU H·ªéI (tr·∫Øc nghi·ªám):
    {question}

    ‚úçÔ∏è TR·∫¢ L·ªúI (nguy√™n vƒÉn + ch·ªçn ƒë√°p √°n ƒë√∫ng):
    """
    )

print("‚úÖ All components initialized.\n")

def rag_query(question: str, use_llm: bool = True):
    if not vectordb or not llm:
        return "‚ö†Ô∏è RAG ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o ƒë√∫ng c√°ch.", ""

    # --- 1Ô∏è‚É£ N·∫øu ng∆∞·ªùi d√πng h·ªèi theo "ƒêi·ªÅu X"
    match = re.search(r"ƒêi·ªÅu\s*(\d+)", question.strip(), re.IGNORECASE)
    if match:
        article_num = match.group(1).strip()
        all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)

        found_docs = []
        for doc, meta in zip(all_data.get("documents", []), all_data.get("metadatas", [])):
            art = meta.get("article", "") if isinstance(meta, dict) else ""
            m = re.search(r"(\d+)", str(art))
            if m and m.group(1).strip() == article_num:
                found_docs.append(f"{art}\n{doc.strip()}")

        if not found_docs:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c ƒëi·ªÅu lu·∫≠t.", f"ƒêi·ªÅu {article_num} (kh√¥ng th·∫•y trong DB)"

        context = "\n---\n".join(found_docs)
        if not use_llm:
            return context, f"ƒêi·ªÅu {article_num} (t√¨m th·∫•y {len(found_docs)} ƒëo·∫°n)"

        # Gi·ªØ c√¢u h·ªèi t·ª± nhi√™n, kh√¥ng √©p prompt n·ªØa
        question = f"N·ªôi dung quy ƒë·ªãnh t·∫°i ƒêi·ªÅu {article_num} l√† g√¨?"

    # --- 2Ô∏è‚É£ Hybrid Search (BM25 + Semantic)
    all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)
    documents = all_data.get("documents", [])
    metadatas = all_data.get("metadatas", [])
    if not documents:
        return "‚ö†Ô∏è CSDL tr·ªëng ho·∫∑c ch∆∞a t·∫£i ƒë√∫ng.", ""

    tokenized_docs = [doc.lower().split() for doc in documents]
    bm25 = BM25Okapi(tokenized_docs)
    bm25_scores = bm25.get_scores(question.lower().split())
    top_bm25_idx = np.argsort(bm25_scores)[::-1][:TOP_K]
    bm25_docs = [(bm25_scores[i], documents[i], metadatas[i]) for i in top_bm25_idx if bm25_scores[i] > 0]

    sem_docs = retriever.invoke(question)

    merged, seen = [], set()
    for score, doc, meta in bm25_docs:
        art = meta.get("article", "Kh√¥ng r√µ") if isinstance(meta, dict) else "Kh√¥ng r√µ"
        if doc not in seen:
            merged.append((float(score), doc, art))
            seen.add(doc)

    q_vec = embed_query_vector(question, embed_tokenizer, embed_model)
    for d in sem_docs:
        d_vec = embed_query_vector(d.page_content, embed_tokenizer, embed_model)
        cos_sim = cosine_similarity(q_vec, d_vec)
        art = d.metadata.get("article", "Kh√¥ng r√µ")
        if d.page_content not in seen and cos_sim >= THRESHOLD:
            merged.append((float(cos_sim), d.page_content, art))
            seen.add(d.page_content)

    if not merged:
        return "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu lu·∫≠t li√™n quan.", ""

    # --- 3Ô∏è‚É£ Ch·ªçn ƒëi·ªÅu c√≥ ƒëi·ªÉm cao nh·∫•t
    merged.sort(key=lambda x: x[0], reverse=True)
    best_score, _, best_art = merged[0]
    same_articles = [doc for score, doc, art in merged if art == best_art]

    cleaned_content = "\n".join(dict.fromkeys("\n".join(same_articles).splitlines()))
    context = f"{best_art}\n{cleaned_content.strip()}"
    if len(context.split()) > 4000:
        context = " ".join(context.split()[:4000])

    if not use_llm:
        return context, f"{best_art} (score={best_score:.2f})"

    # --- 4Ô∏è‚É£ T·∫°o prompt ph√π h·ª£p ---
    if re.search(r"\b[a-e]\)", question.lower()):
        prompt_text = prompt_template_quiz.format(context=context, question=question)
    else:
        prompt_text = prompt_template_normal.format(context=context, question=question)

    try:
        result = llm(prompt_text,max_new_tokens=512,do_sample=True,temperature=0.1,top_p=0.8)
        answer = result[0]["generated_text"].strip()
        answer = re.sub(r'(?i)assistant[:Ôºö-]*\s*', '', answer).strip()
        # ‚ùå C·∫Øt ph·∫ßn "Explanation" ho·∫∑c "Gi·∫£i th√≠ch" n·∫øu c√≥
        answer = re.split(r"(###?\s*Explanation:|Gi·∫£i th√≠ch[:Ôºö])", answer, flags=re.IGNORECASE)[0].strip()

        # ‚ùå C·∫Øt ph·∫ßn "Answer:" n·∫øu c√≥ ti√™u ƒë·ªÅ
        answer = re.sub(r"^###?\s*Answer:\s*", "", answer, flags=re.IGNORECASE).strip()

        # ‚ùå Lo·∫°i b·ªè ti√™u ƒë·ªÅ "Tr·∫£ l·ªùi" ho·∫∑c ph·∫ßn l·∫∑p l·∫°i
        answer = re.sub(r"(?i)(###?\s*tr·∫£ l·ªùi[:Ôºö]*\s*)", "", answer).strip()

        # ‚úÖ C·∫Øt b·ªè ph·∫ßn tr√πng l·∫∑p n·∫øu m√¥ h√¨nh l·∫∑p n·ªôi dung nhi·ªÅu l·∫ßn
        lines = [line.strip() for line in answer.splitlines() if line.strip()]
        unique_lines = []
        for line in lines:
            if line not in unique_lines:
                unique_lines.append(line)

        # ‚úÖ Gi·ªØ l·∫°i t·ªëi ƒëa 1 ƒëo·∫°n n·ªôi dung tr√πng l·∫∑p (tr√°nh 5‚Äì6 l·∫ßn l·∫∑p y h·ªát)
        answer = "\n".join(unique_lines)

        # ‚úÖ N·∫øu m√¥ h√¨nh t·ª± sinh nhi·ªÅu kh·ªëi ‚Äú---‚Äù, c·∫Øt ph·∫ßn ƒë·∫ßu ti√™n
        answer = answer.split('---')[0].strip()

        # ‚úÖ N·∫øu m√¥ h√¨nh l·∫∑p l·∫°i to√†n b·ªô block nhi·ªÅu l·∫ßn, c·∫Øt ph·∫ßn l·∫∑p d·ª±a tr√™n d√≤ng ƒë·∫ßu ti√™n
        if answer.count(unique_lines[0]) > 1:
            first = answer.find(unique_lines[0])
            second = answer.find(unique_lines[0], first + len(unique_lines[0]))
            if second != -1:
                answer = answer[:second].strip()

    except Exception as e:
        answer = f"L·ªói khi sinh c√¢u tr·∫£ l·ªùi: {e}"

    # N·∫øu LLM kh√¥ng tr√≠ch ƒë∆∞·ª£c ‚Äî tr·∫£ context ƒë·ªÉ debug
    if not answer or "kh√¥ng t√¨m th·∫•y" in answer.lower():
        return context, f"[DEBUG: LLM kh√¥ng tr√≠ch ƒë∆∞·ª£c] {best_art} (score={best_score:.2f})"

    return answer, f"{best_art} (score={best_score:.2f})"

try:
    initialize_rag_components()
except Exception as e:
    print(f"‚ùå L·ªñI KH·ªûI T·∫†O NGHI√äM TR·ªåNG: {e}")


# ======= Gradio UI =======
with gr.Blocks(title="‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam (Qwen 3B RAG)") as demo:
    gr.Markdown(f"""
    ## ‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam
    **LLM:** `{LLM_MODEL}`  
    **Embedding:** `{EMBED_MODEL}`  
    ---
    """)

    with gr.Row():
        with gr.Column(scale=2):
            question = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi ph√°p l√Ω:", lines=3,
                                  placeholder="V√≠ d·ª•: ƒêi·ªÅu 47 quy ƒë·ªãnh g√¨ v·ªÅ thu·ªëc generic?")
            use_llm = gr.Checkbox(label="G·ªçi LLM (b·∫≠t ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi)", value=True)
            ask = gr.Button("H·ªèi", variant="primary")
            clear = gr.Button("Xo√°")
        with gr.Column(scale=3):
            answer_box = gr.Textbox(label="Tr·∫£ l·ªùi", lines=10, interactive=False)
            source_box = gr.Textbox(label="ƒêi·ªÅu lu·∫≠t tr√≠ch d·∫´n", lines=6, interactive=False)

    ask.click(fn=rag_query, inputs=[question, use_llm], outputs=[answer_box, source_box])
    clear.click(lambda: ("", "", ""), outputs=[question, answer_box, source_box])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
