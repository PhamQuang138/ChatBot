#!/usr/bin/env python3
import os
import re
import json
import torch
import numpy as np
import gradio as gr
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModel,
    BitsAndBytesConfig,
    pipeline
)
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate


# ================== CONFIG ==================
BASE_DIR = "/home/quang/Documents/ChatBot"
CHROMA_PATH = os.path.join(BASE_DIR, "data", "chroma_db_qwen_embed_vn")
LLM_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
EMBED_MODEL = "Qwen/Qwen3-Embedding-0.6B"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOP_K = 10
THRESHOLD = 0.1
BATCH_SIZE = 4
# ===========================================


# ===== Embedding wrapper =====
class Qwen3Embedding(Embeddings):
    def __init__(self, model, tokenizer, device="cpu", batch_size=4):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.batch_size = batch_size

    def embed_documents(self, texts):
        all_embs = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True,
                                    truncation=True, max_length=512).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
                vecs = outputs.last_hidden_state[:, 0, :]
                vecs = vecs / (vecs.norm(dim=-1, keepdim=True) + 1e-12)
                all_embs.append(vecs.cpu().numpy())
        if not all_embs:
            return np.zeros((0, self.model.config.hidden_size)).tolist()
        return np.vstack(all_embs).tolist()

    def embed_query(self, text):
        return self.embed_documents([text])[0]


# ===== Utility =====
def cosine_similarity(a, b):
    a, b = np.asarray(a), np.asarray(b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    return float(np.dot(a, b) / denom) if denom != 0 else 0.0


def embed_query_vector(text: str, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)
        emb = outputs.last_hidden_state[:, 0, :]
        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-12)
    return emb.cpu().numpy()[0]


# ===== Initialization =====
vectordb = None
retriever = None
llm = None
prompt_template = None
embed_model = None
embed_tokenizer = None
embedding_fn = None

# ===== Initialization =====
def initialize_rag_components():
    global vectordb, retriever, llm, prompt_template, embed_model, embed_tokenizer, embedding_fn

    print("üõ†Ô∏è Initializing RAG components...")

    # 1Ô∏è‚É£ Load embedding model
    print(f"üîπ Loading embedding model: {EMBED_MODEL}")
    embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)
    embed_model = AutoModel.from_pretrained(EMBED_MODEL, device_map="auto", torch_dtype=torch.float16)
    # ensure model is on the expected device for later inference
    try:
        embed_model.to(DEVICE)
    except Exception:
        # fallback: some HF models with device_map="auto" may not accept .to() - ignore if fails
        pass
    embedding_fn = Qwen3Embedding(embed_model, embed_tokenizer, DEVICE, BATCH_SIZE)
    print("‚úÖ Embedding model ready.")

    # 2Ô∏è‚É£ Load Chroma DB
    if not os.path.exists(CHROMA_PATH):
        raise FileNotFoundError(f"‚ùå Chroma DB not found at {CHROMA_PATH}")
    print(f"üîπ Loading Chroma DB from {CHROMA_PATH}")
    vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_fn)
    retriever = vectordb.as_retriever(search_kwargs={"k": TOP_K})
    print("‚úÖ Chroma retriever ready.")

    # 3Ô∏è‚É£ Load LLM
    print(f"üîπ Loading LLM {LLM_MODEL} (8-bit)...")
    bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)
    tokenizer_llm = AutoTokenizer.from_pretrained(LLM_MODEL)
    model_llm = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL, device_map="auto", torch_dtype=torch.float16, quantization_config=bnb_config
    )
    llm_pipe = pipeline("text-generation", model=model_llm, tokenizer=tokenizer_llm,
                        max_new_tokens=512, do_sample=False, return_full_text=False)
    llm = HuggingFacePipeline(pipeline=llm_pipe)
    print("‚úÖ LLM ready.")

    # 4Ô∏è‚É£ Prompt Template (phi√™n b·∫£n c·ª±c nghi√™m ng·∫∑t)
    prompt_template = ChatPromptTemplate.from_template(
        """B·∫°n l√† **tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ Lu·∫≠t D∆∞·ª£c Vi·ªát Nam**.

QUY T·∫ÆC NGHI√äM NG·∫∂T:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n **n·ªôi dung ƒëi·ªÅu lu·∫≠t trong CONTEXT** b√™n d∆∞·ªõi.
- N·∫øu **kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p**, ph·∫£i tr·∫£ l·ªùi ƒë√∫ng c√¢u n√†y:
  üëâ "Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c ƒëi·ªÅu lu·∫≠t ƒë∆∞·ª£c cung c·∫•p."
- Kh√¥ng ƒë∆∞·ª£c th√™m b·∫•t k·ª≥ c√¢u xin l·ªói, suy lu·∫≠n hay l·ªùi gi·∫£i th√≠ch n√†o kh√°c.

---
### CONTEXT (C√°c ƒëi·ªÅu lu·∫≠t li√™n quan)
{context}

### C√ÇU H·ªéI
{question}

### TR·∫¢ L·ªúI (ng·∫Øn g·ªçn, chu·∫©n ph√°p l√Ω, ti·∫øng Vi·ªát)
"""
    )

    print("‚úÖ All components initialized.\n")


def rag_query(question: str):
    if not vectordb or not llm:
        return "‚ö†Ô∏è RAG ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o ƒë√∫ng c√°ch.", ""

    # === N·∫øu c√≥ d·∫°ng "ƒêi·ªÅu X" ===
    match = re.search(r"ƒêi·ªÅu\s*(\d+)", question, re.IGNORECASE)
    if match:
        article_num = match.group(1).strip()
        all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)

        found_docs = []
        for doc, meta in zip(all_data.get("documents", []), all_data.get("metadatas", [])):
            # --- L·∫•y th√¥ng tin article ---
            art = ""
            if isinstance(meta, dict):
                art = meta.get("article", "") or meta.get("source", "")
            if not art:
                continue

            # --- So kh·ªõp theo s·ªë ƒëi·ªÅu ---
            m = re.search(r"(\d+)", str(art))
            if m and m.group(1).strip() == article_num:
                # ‚úÖ N·ªôi dung n·∫±m trong `documents`, kh√¥ng ph·∫£i `meta["content"]`
                content_text = doc
                if content_text:
                    found_docs.append(f"{art}\n{content_text.strip()}")

        # --- Kh√¥ng t√¨m th·∫•y ---
        if not found_docs:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c ƒëi·ªÅu lu·∫≠t ƒë∆∞·ª£c cung c·∫•p.", f"ƒêi·ªÅu {article_num} (kh√¥ng th·∫•y trong DB)"

        # --- Gh√©p context v√† g·ªçi LLM ---
        context = "\n---\n".join(found_docs)
        prompt = prompt_template.format(context=context, question=question)
        print("\n===== DEBUG PROMPT =====\n", prompt[:1500], "\n=========================\n")

        answer = llm.invoke(prompt).strip()

        if not answer or len(answer) < 5:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c ƒëi·ªÅu lu·∫≠t ƒë∆∞·ª£c cung c·∫•p.", f"ƒêi·ªÅu {article_num} (c√≥ {len(found_docs)} ƒëo·∫°n)"
        return answer, f"ƒêi·ªÅu {article_num} (t√¨m th·∫•y {len(found_docs)} ƒëo·∫°n)"

    # === N·∫øu l√† c√¢u h·ªèi t·ª± nhi√™n ===
    docs = retriever.invoke(question)
    if not docs:
        return "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu lu·∫≠t li√™n quan.", ""

    q_vec = embed_query_vector(question, embed_tokenizer, embed_model)
    ranked_docs = []
    for d in docs:
        d_inputs = embed_tokenizer(d.page_content, return_tensors="pt", truncation=True,
                                   padding=True, max_length=512).to(DEVICE)
        with torch.no_grad():
            outputs = embed_model(**d_inputs)
            vec = outputs.last_hidden_state[:, 0, :]
            vec = vec / vec.norm(dim=-1, keepdim=True)
        d_vec = vec.cpu().numpy()[0]
        score = cosine_similarity(q_vec, d_vec)
        if score >= THRESHOLD:
            ranked_docs.append((score, d))

    ranked_docs.sort(key=lambda x: x[0], reverse=True)
    if not ranked_docs:
        return "‚ö†Ô∏è Kh√¥ng c√≥ ƒëi·ªÅu lu·∫≠t n√†o v∆∞·ª£t ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng.", ""

    context_blocks, sources = [], []
    for score, d in ranked_docs[:3]:
        art = d.metadata.get("article", "Kh√¥ng r√µ")
        context_blocks.append(f"[{score:.2f}] {art}\n{d.page_content}")
        sources.append(f"{art} (ƒë·ªô t∆∞∆°ng ƒë·ªìng={score:.2f})")

    context = "\n---\n".join(context_blocks)
    prompt = prompt_template.format(context=context, question=question)
    answer = llm.invoke(prompt).strip()

    if not answer or "kh√¥ng t√¨m th·∫•y" in answer.lower() or len(answer) < 5:
        answer = "Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c ƒëi·ªÅu lu·∫≠t ƒë∆∞·ª£c cung c·∫•p."

    return answer, "\n".join(sources)



# ======= Startup =======
try:
    initialize_rag_components()
except Exception as e:
    print(f"‚ùå L·ªñI KH·ªûI T·∫†O NGHI√äM TR·ªåNG: {e}")


# ======= Gradio UI =======
with gr.Blocks(title="‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam (Qwen RAG)") as demo:
    gr.Markdown(f"""
    ## ‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam
    **LLM:** `{LLM_MODEL}`  
    **Embedding:** `{EMBED_MODEL}`  
    **Thi·∫øt b·ªã:** `{DEVICE}`  
    **CSDL:** `{CHROMA_PATH}`
    ---
    """)

    with gr.Row():
        with gr.Column(scale=2):
            question = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi ph√°p l√Ω:", lines=3,
                                  placeholder="V√≠ d·ª•: ƒêi·ªÅu 47 quy ƒë·ªãnh g√¨ v·ªÅ thu·ªëc generic?")
            ask = gr.Button("H·ªèi", variant="primary")
            clear = gr.Button("Xo√°")
        with gr.Column(scale=3):
            answer_box = gr.Textbox(label="Tr·∫£ l·ªùi", lines=10, interactive=False)
            source_box = gr.Textbox(label="ƒêi·ªÅu lu·∫≠t tr√≠ch d·∫´n", lines=6, interactive=False)

    ask.click(fn=rag_query, inputs=question, outputs=[answer_box, source_box])
    clear.click(lambda: ("", "", ""), outputs=[question, answer_box, source_box])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
