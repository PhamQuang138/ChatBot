#!/usr/bin/env python3
import os
import re
import json
import torch
import numpy as np
import gradio as gr
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModel,
    BitsAndBytesConfig,
    pipeline
)
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate
from rank_bm25 import BM25Okapi

# ================== CONFIG ==================
BASE_DIR = "/home/quang/Documents/ChatBot"
CHROMA_PATH = os.path.join(BASE_DIR, "data", "chroma_db_qwen_embed_vn")
LLM_MODEL = "meta-llama/Llama-3.2-1B"      # ‚úÖ model base ƒë√∫ng c·ªßa LoRA fine-tuned
EMBED_MODEL = "Qwen/Qwen3-Embedding-0.6B"  # v·∫´n c√≥ th·ªÉ d√πng Qwen3 embed
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOP_K = 20
THRESHOLD = 0.005
BATCH_SIZE = 4

import difflib

def remove_near_duplicates(lines, similarity=0.9):
    cleaned = []
    for line in lines:
        if not cleaned:
            cleaned.append(line)
            continue
        sim = difflib.SequenceMatcher(None, cleaned[-1], line).ratio()
        if sim < similarity:
            cleaned.append(line)
    return cleaned

# ===== Embedding wrapper =====
class Qwen3Embedding(Embeddings):
    def __init__(self, model, tokenizer, device="cpu", batch_size=4):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.batch_size = batch_size

    def embed_documents(self, texts):
        all_embs = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True,
                                    truncation=True, max_length=512).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
                vecs = outputs.last_hidden_state[:, 0, :]
                vecs = vecs / (vecs.norm(dim=-1, keepdim=True) + 1e-12)
                all_embs.append(vecs.cpu().numpy())
        if not all_embs:
            return np.zeros((0, self.model.config.hidden_size)).tolist()
        return np.vstack(all_embs).tolist()

    def embed_query(self, text):
        return self.embed_documents([text])[0]


# ===== Utility =====
def cosine_similarity(a, b):
    a, b = np.asarray(a), np.asarray(b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    return float(np.dot(a, b) / denom) if denom != 0 else 0.0


def embed_query_vector(text: str, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)
        emb = outputs.last_hidden_state[:, 0, :]
        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-12)
    return emb.cpu().numpy()[0]


# ===== Initialization =====
vectordb = None
retriever = None
llm = None
prompt_template = None
embed_model = None
embed_tokenizer = None
embedding_fn = None

def initialize_rag_components():
    global vectordb, retriever, llm, prompt_template, embed_model, embed_tokenizer, embedding_fn

    print("üõ†Ô∏è Initializing RAG components...")

    # 1Ô∏è‚É£ Load embedding model
    print(f"üîπ Loading embedding model: {EMBED_MODEL}")
    embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)
    embed_model = AutoModel.from_pretrained(
        EMBED_MODEL,
        device_map="auto",
        torch_dtype=torch.float16
    )
    try:
        embed_model.to(DEVICE)
    except Exception:
        pass
    embedding_fn = Qwen3Embedding(embed_model, embed_tokenizer, DEVICE, BATCH_SIZE)
    print("‚úÖ Embedding model ready.")

    # 2Ô∏è‚É£ Load Chroma DB
    if not os.path.exists(CHROMA_PATH):
        raise FileNotFoundError(f"‚ùå Chroma DB not found at {CHROMA_PATH}")
    print(f"üîπ Loading Chroma DB from {CHROMA_PATH}")
    vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_fn)
    retriever = vectordb.as_retriever(search_kwargs={"k": TOP_K})
    print("‚úÖ Chroma retriever ready.")

    # 3Ô∏è‚É£ Load LLM (base Llama + LoRA)
    print(f"üîπ Loading base LLM: {LLM_MODEL} (4-bit)...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )

    tokenizer_llm = AutoTokenizer.from_pretrained(LLM_MODEL)
    model_llm = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        quantization_config=bnb_config,
        low_cpu_mem_usage=True
    )

    # ‚ö° Load LoRA adapter
    lora_path = os.path.join(BASE_DIR, "src", "lora_llama3_4bit")
    if os.path.exists(lora_path):
        try:
            from peft import PeftModel
            print(f"üîπ Attaching LoRA adapter from {lora_path}")
            model_llm = PeftModel.from_pretrained(model_llm, lora_path)
            print("‚úÖ LoRA adapter loaded successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Kh√¥ng th·ªÉ load LoRA adapter: {e}")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c LoRA t·∫°i {lora_path}")

    llm = pipeline(
        "text-generation",
        model=model_llm,
        tokenizer=tokenizer_llm,
        return_full_text=False,
        no_repeat_ngram_size=6
    )

    print("‚úÖ LLM ready.")


prompt_template_normal = ChatPromptTemplate.from_template(
    """B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ **Lu·∫≠t D∆∞·ª£c Vi·ªát Nam**.

D·ª±a **ch·ªâ tr√™n ph·∫ßn CONTEXT d∆∞·ªõi ƒë√¢y**, h√£y **tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh ph√°p lu·∫≠t** c√≥ li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
- N·∫øu trong ph·∫ßn CONTEXT c√≥ c√°c c√¢u ƒë√°nh s·ªë (1, 2, 3...) ho·∫∑c a),b),c),...h√£y tr√¨nh b√†y xu·ªëng d√≤ng r√µ r√†ng.

Tuy·ªát ƒë·ªëi **kh√¥ng ƒë∆∞·ª£c suy lu·∫≠n, di·ªÖn gi·∫£i, ho·∫∑c paraphrase**.

- N·∫øu kh√¥ng c√≥ n·ªôi dung n√†o trong CONTEXT tr√πng kh·ªõp ho·∫∑c tr√≠ch d·∫´n nguy√™n vƒÉn ƒëi·ªÅu lu·∫≠t, **d√π c√≥ c√°c c√¢u t∆∞∆°ng t·ª± ho·∫∑c di·ªÖn gi·∫£i**, th√¨ ph·∫£i tr·∫£ ƒë√∫ng duy nh·∫•t:
  "Kh√¥ng t√¨m th·∫•y quy ƒë·ªãnh li√™n quan trong CONTEXT."

- N·∫øu c√≥ nhi·ªÅu ƒëo·∫°n gi·ªëng nhau ho·∫∑c tr√πng l·∫∑p, ch·ªâ gi·ªØ l·∫°i **m·ªôt b·∫£n ƒë·∫ßy ƒë·ªß nh·∫•t**.

---
üìë CONTEXT:
{context}

üí¨ C√ÇU H·ªéI:
{question}

‚úçÔ∏è TR·∫¢ L·ªúI (tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh ho·∫∑c c√¢u th√¥ng b√°o tr√™n):
"""
)


prompt_template_quiz = ChatPromptTemplate.from_template(
    """B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ **Lu·∫≠t D∆∞·ª£c Vi·ªát Nam**.

C√¢u h·ªèi sau ƒë√¢y c√≥ d·∫°ng **tr·∫Øc nghi·ªám nhi·ªÅu l·ª±a ch·ªçn** (a, b, c, d...).
D·ª±a **ch·ªâ tr√™n ph·∫ßn CONTEXT**, h√£y:
- tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh li√™n quan,
- kh√¥ng th√™m gi·∫£i th√≠ch hay b√¨nh lu·∫≠n.
- N·∫øu trong ph·∫ßn CONTEXT c√≥ c√°c c√¢u ƒë√°nh s·ªë (1, 2, 3...) ho·∫∑c a),b),c),...h√£y tr√¨nh b√†y xu·ªëng d√≤ng r√µ r√†ng.


Kh√¥ng ƒë∆∞·ª£c:
- T·ª± t·∫°o n·ªôi dung, URL, hay s·ªë li·ªáu.
- D·ªãch sang ng√¥n ng·ªØ kh√°c.
- Th√™m b√¨nh lu·∫≠n, gi·∫£i th√≠ch hay suy lu·∫≠n.

---
üìò CONTEXT:
{context}

üí¨ C√ÇU H·ªéI (tr·∫Øc nghi·ªám):
{question}

‚úçÔ∏è TR·∫¢ L·ªúI (nguy√™n vƒÉn + ch·ªçn ƒë√°p √°n ƒë√∫ng):
"""
)

print("‚úÖ All components initialized.\n")

# ============= RAG QUERY =============
def rag_query(question: str, use_llm: bool = True):
    if not vectordb or not llm:
        return "‚ö†Ô∏è RAG ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o ƒë√∫ng c√°ch.", ""

    # === 1Ô∏è‚É£ N·∫øu c√¢u h·ªèi c√≥ ch·ª©a 'ƒêi·ªÅu X' ‚Üí ch·ªâ truy xu·∫•t d·ªØ li·ªáu, kh√¥ng g·ªçi LLM ===
    match = re.search(r"ƒêi·ªÅu\s*(\d+)", question.strip(), re.IGNORECASE)
    if match:
        article_num = match.group(1).strip()
        all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)

        found_docs = []
        for doc, meta in zip(all_data.get("documents", []), all_data.get("metadatas", [])):
            art = meta.get("article", "") if isinstance(meta, dict) else ""
            m = re.search(r"(\d+)", str(art))
            if m and m.group(1).strip() == article_num:
                found_docs.append(f"{art}\n{doc.strip()}")

        if not found_docs:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c ƒëi·ªÅu lu·∫≠t.", f"ƒêi·ªÅu {article_num} (kh√¥ng th·∫•y trong DB)"

        context = "\n---\n".join(found_docs)

        # üö´ T·ª± ƒë·ªông b·ªè qua LLM n·∫øu c√¢u h·ªèi ch·ªâ d·∫°ng 'ƒêi·ªÅu X' ho·∫∑c t∆∞∆°ng t·ª±
        if re.fullmatch(r".*ƒêi·ªÅu\s*\d+.*", question.strip(), re.IGNORECASE):
            return context, f"ƒêi·ªÅu {article_num} (t√¨m th·∫•y {len(found_docs)} ƒëo·∫°n)"

        # N·∫øu c√¢u h·ªèi d√†i ho·∫∑c c√≥ th√™m n·ªôi dung ‚Üí v·∫´n c√≥ th·ªÉ g·ªçi LLM
        if not use_llm:
            return context, f"ƒêi·ªÅu {article_num} (t√¨m th·∫•y {len(found_docs)} ƒëo·∫°n)"

        question = f"N·ªôi dung quy ƒë·ªãnh t·∫°i ƒêi·ªÅu {article_num} l√† g√¨?"

    # === 2Ô∏è‚É£ Hybrid Search (BM25 + Embedding) ===
    all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)
    documents = all_data.get("documents", [])
    metadatas = all_data.get("metadatas", [])
    if not documents:
        return "‚ö†Ô∏è CSDL tr·ªëng ho·∫∑c ch∆∞a t·∫£i ƒë√∫ng.", ""

    tokenized_docs = [doc.lower().split() for doc in documents]
    bm25 = BM25Okapi(tokenized_docs)
    bm25_scores = bm25.get_scores(question.lower().split())
    top_bm25_idx = np.argsort(bm25_scores)[::-1][:TOP_K]
    bm25_docs = [(bm25_scores[i], documents[i], metadatas[i]) for i in top_bm25_idx if bm25_scores[i] > 0]

    sem_docs = retriever.invoke(question)
    merged, seen = [], set()

    for score, doc, meta in bm25_docs:
        art = meta.get("article", "Kh√¥ng r√µ") if isinstance(meta, dict) else "Kh√¥ng r√µ"
        if doc not in seen:
            merged.append((float(score), doc, art))
            seen.add(doc)

    q_vec = embed_query_vector(question, embed_tokenizer, embed_model)
    for d in sem_docs:
        d_vec = embed_query_vector(d.page_content, embed_tokenizer, embed_model)
        cos_sim = cosine_similarity(q_vec, d_vec)
        art = d.metadata.get("article", "Kh√¥ng r√µ")
        if d.page_content not in seen and cos_sim >= THRESHOLD:
            merged.append((float(cos_sim), d.page_content, art))
            seen.add(d.page_content)

    if not merged:
        return "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu lu·∫≠t li√™n quan.", ""

    merged.sort(key=lambda x: x[0], reverse=True)
    best_score, _, best_art = merged[0]
    same_articles = [doc for score, doc, art in merged if art == best_art]
    context = f"{best_art}\n" + "\n".join(same_articles).strip()

    if len(context.split()) > 4000:
        context = " ".join(context.split()[:4000])

    # N·∫øu kh√¥ng mu·ªën g·ªçi LLM th√¨ tr·∫£ l·∫°i lu√¥n context
    if not use_llm:
        return context, f"{best_art} (score={best_score:.2f})"

    # === 3Ô∏è‚É£ G·ªçi LLM n·∫øu c·∫ßn ===
    prompt_text = (
        prompt_template_quiz.format(context=context, question=question)
        if re.search(r"\b[a-e]\)", question.lower())
        else prompt_template_normal.format(context=context, question=question)
    )

    try:
        result = llm(prompt_text, max_new_tokens=512)
        answer = result[0]["generated_text"].strip()

        lines = [line.strip() for line in answer.splitlines() if line.strip()]
        unique_lines = remove_near_duplicates(lines, similarity=0.9)
        answer = " ".join(unique_lines)

    except Exception as e:
        answer = f"L·ªói khi sinh c√¢u tr·∫£ l·ªùi: {e}"

    return answer, f"{best_art} (score={best_score:.2f})"


# ======= Gradio UI =======
try:
    initialize_rag_components()
except Exception as e:
    print(f"‚ùå L·ªñI KH·ªûI T·∫†O NGHI√äM TR·ªåNG: {e}")

with gr.Blocks(title="‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam (Llama 1B LoRA)") as demo:
    gr.Markdown(f"""
    ## ‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam
    **LLM:** `{LLM_MODEL}`  
    **Embedding:** `{EMBED_MODEL}`  
    ---
    """)

    with gr.Row():
        with gr.Column(scale=2):
            question = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi ph√°p l√Ω:", lines=3,
                                  placeholder="V√≠ d·ª•: ƒêi·ªÅu 47 quy ƒë·ªãnh g√¨ v·ªÅ thu·ªëc generic?")
            use_llm = gr.Checkbox(label="G·ªçi LLM (b·∫≠t ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi)", value=True)
            ask = gr.Button("H·ªèi", variant="primary")
            clear = gr.Button("Xo√°")
        with gr.Column(scale=3):
            answer_box = gr.Textbox(label="Tr·∫£ l·ªùi", lines=10, interactive=False)
            source_box = gr.Textbox(label="ƒêi·ªÅu lu·∫≠t tr√≠ch d·∫´n", lines=6, interactive=False)

    ask.click(fn=rag_query, inputs=[question, use_llm], outputs=[answer_box, source_box])
    clear.click(lambda: ("", "", ""), outputs=[question, answer_box, source_box])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
