#!/usr/bin/env python3
import os
import re
import json
import torch
import numpy as np
import gradio as gr
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModel,
    BitsAndBytesConfig,
    pipeline
)
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate
from rank_bm25 import BM25Okapi

# ================== CONFIG ==================
BASE_DIR = "/home/quang/Documents/ChatBot"
CHROMA_PATH = os.path.join(BASE_DIR, "data", "chroma_db_qwen_embed_vn")
LLM_MODEL = "meta-llama/Llama-3.2-1B"  # model base ƒë√∫ng c·ªßa LoRA fine-tuned
EMBED_MODEL = "Qwen/Qwen3-Embedding-0.6B"  # v·∫´n c√≥ th·ªÉ d√πng Qwen3 embed
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOP_K = 20
THRESHOLD = 0.005
BATCH_SIZE = 4

import difflib


def remove_near_duplicates(lines, similarity=0.9):
    cleaned = []
    for line in lines:
        if not line: # B·ªè qua d√≤ng tr·ªëng
            continue
        is_duplicate = False
        # So s√°nh d√≤ng hi·ªán t·∫°i v·ªõi T·∫§T C·∫¢ c√°c d√≤ng ƒë√£ ƒë∆∞·ª£c th√™m
        for existing_line in cleaned:
            sim = difflib.SequenceMatcher(None, existing_line, line).ratio()
            if sim >= similarity:
                is_duplicate = True
                break # D√≤ng n√†y l·∫∑p, b·ªè qua

        if not is_duplicate:
            cleaned.append(line)
    return cleaned

# ===== Embedding wrapper =====
class Qwen3Embedding(Embeddings):
    def __init__(self, model, tokenizer, device="cpu", batch_size=4):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.batch_size = batch_size

    def embed_documents(self, texts):
        all_embs = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True,
                                    truncation=True, max_length=512).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
                vecs = outputs.last_hidden_state[:, 0, :]
                vecs = vecs / (vecs.norm(dim=-1, keepdim=True) + 1e-12)
                all_embs.append(vecs.cpu().numpy())
        if not all_embs:
            return np.zeros((0, self.model.config.hidden_size)).tolist()
        return np.vstack(all_embs).tolist()

    def embed_query(self, text):
        return self.embed_documents([text])[0]


# ===== Utility =====
def cosine_similarity(a, b):
    a, b = np.asarray(a), np.asarray(b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    return float(np.dot(a, b) / denom) if denom != 0 else 0.0


def embed_query_vector(text: str, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)
        emb = outputs.last_hidden_state[:, 0, :]
        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-12)
    return emb.cpu().numpy()[0]


# ===== Initialization =====
vectordb = None
retriever = None
llm = None
prompt_template = None
embed_model = None
embed_tokenizer = None
embedding_fn = None


def initialize_rag_components():
    global vectordb, retriever, llm, prompt_template, embed_model, embed_tokenizer, embedding_fn

    print("üõ†Ô∏è Initializing RAG components...")

    # 1Ô∏è‚É£ Load embedding model
    print(f"üîπ Loading embedding model: {EMBED_MODEL}")
    embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)
    embed_model = AutoModel.from_pretrained(
        EMBED_MODEL,
        device_map="auto",
        torch_dtype=torch.float16
    )
    try:
        embed_model.to(DEVICE)
    except Exception:
        pass
    embedding_fn = Qwen3Embedding(embed_model, embed_tokenizer, DEVICE, BATCH_SIZE)
    print("‚úÖ Embedding model ready.")

    # 2Ô∏è‚É£ Load Chroma DB
    if not os.path.exists(CHROMA_PATH):
        raise FileNotFoundError(f"‚ùå Chroma DB not found at {CHROMA_PATH}")
    print(f"üîπ Loading Chroma DB from {CHROMA_PATH}")
    vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_fn)
    retriever = vectordb.as_retriever(search_kwargs={"k": TOP_K})
    print("‚úÖ Chroma retriever ready.")

    # 3Ô∏è‚É£ Load LLM (base Llama + LoRA)
    print(f"üîπ Loading base LLM: {LLM_MODEL} (4-bit)...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )

    tokenizer_llm = AutoTokenizer.from_pretrained(LLM_MODEL)
    model_llm = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        quantization_config=bnb_config,
        low_cpu_mem_usage=True
    )

    # ‚ö° Load LoRA adapter
    lora_path = os.path.join(BASE_DIR, "src", "lora_llama3_4bit")
    if os.path.exists(lora_path):
        try:
            from peft import PeftModel
            print(f"üîπ Attaching LoRA adapter from {lora_path}")
            model_llm = PeftModel.from_pretrained(model_llm, lora_path)
            print("‚úÖ LoRA adapter loaded successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Kh√¥ng th·ªÉ load LoRA adapter: {e}")
    else:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c LoRA t·∫°i {lora_path}")

    llm = pipeline(
        "text-generation",
        model=model_llm,
        tokenizer=tokenizer_llm,
        return_full_text=False,
        no_repeat_ngram_size=6
    )

    print("‚úÖ LLM ready.")


prompt_template_normal = ChatPromptTemplate.from_template(
    """B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ **Lu·∫≠t D∆∞·ª£c Vi·ªát Nam**.

D·ª±a **ch·ªâ tr√™n ph·∫ßn CONTEXT d∆∞·ªõi ƒë√¢y**, h√£y **tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh ph√°p lu·∫≠t** c√≥ li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
- N·∫øu trong ph·∫ßn CONTEXT c√≥ c√°c c√¢u ƒë√°nh s·ªë (1, 2, 3...) ho·∫∑c a),b),c),...h√£y tr√¨nh b√†y xu·ªëng d√≤ng r√µ r√†ng.
B·∫Øt bu·ªôc **ch·ªâ tr·∫£ l·ªùi b·∫±ng ti·∫øng vi·ªát **.
Nghi√™m c·∫•m **kh√¥ng ƒë∆∞·ª£c suy lu·∫≠n, di·ªÖn gi·∫£i, b·ªãa ƒë·∫∑t, c·∫•m th√™m icon **.


- N·∫øu c√≥ nhi·ªÅu ƒëo·∫°n gi·ªëng nhau ho·∫∑c tr√πng l·∫∑p, ch·ªâ gi·ªØ l·∫°i **m·ªôt b·∫£n ƒë·∫ßy ƒë·ªß nh·∫•t**.

---
üìë CONTEXT:
{context}

üí¨ C√ÇU H·ªéI:
{question}

‚úçÔ∏è TR·∫¢ L·ªúI (tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh ho·∫∑c c√¢u th√¥ng b√°o tr√™n):
"""
)

prompt_template_quiz = ChatPromptTemplate.from_template(
    """B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω chuy√™n v·ªÅ **Lu·∫≠t D∆∞·ª£c Vi·ªát Nam**.

C√¢u h·ªèi sau ƒë√¢y c√≥ d·∫°ng **tr·∫Øc nghi·ªám nhi·ªÅu l·ª±a ch·ªçn** (a, b, c, d...).
D·ª±a **ch·ªâ tr√™n ph·∫ßn CONTEXT**, h√£y:
- tr√≠ch nguy√™n vƒÉn quy ƒë·ªãnh li√™n quan,
- kh√¥ng th√™m gi·∫£i th√≠ch hay b√¨nh lu·∫≠n.
- N·∫øu trong ph·∫ßn CONTEXT c√≥ c√°c c√¢u ƒë√°nh s·ªë (1, 2, 3...) ho·∫∑c a),b),c),...h√£y tr√¨nh b√†y xu·ªëng d√≤ng r√µ r√†ng.


Kh√¥ng ƒë∆∞·ª£c:
- T·ª± t·∫°o n·ªôi dung, URL, hay s·ªë li·ªáu.
- D·ªãch sang ng√¥n ng·ªØ kh√°c.
- Th√™m b√¨nh lu·∫≠n, gi·∫£i th√≠ch hay suy lu·∫≠n.

---
üìò CONTEXT:
{context}

üí¨ C√ÇU H·ªéI (tr·∫Øc nghi·ªám):
{question}

‚úçÔ∏è TR·∫¢ L·ªúI (nguy√™n vƒÉn + ch·ªçn ƒë√°p √°n ƒë√∫ng):
"""
)

print("‚úÖ All components initialized.\n")


# ============= RAG QUERY =============
def rag_query(question: str, use_llm: bool = True):
    if not vectordb or not llm:
        return "‚ö†Ô∏è RAG ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o ƒë√∫ng c√°ch.", "", "N/A"

    final_context = ""
    source_info = ""
    metrics_str = ""
    q_vec = None  # S·∫Ω d√πng ƒë·ªÉ t√≠nh to√°n

    # === 1Ô∏è‚É£ N·∫øu c√¢u h·ªèi c√≥ ch·ª©a 'ƒêi·ªÅu X' ‚Üí ch·ªâ truy xu·∫•t d·ªØ li·ªáu ===
    match = re.search(r"ƒêi·ªÅu\s*(\d+)", question.strip(), re.IGNORECASE)
    if match:
        article_num = match.group(1).strip()
        all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)

        found_docs = []
        for doc, meta in zip(all_data.get("documents", []), all_data.get("metadatas", [])):
            art = meta.get("article", "") if isinstance(meta, dict) else ""
            m = re.search(r"(\d+)", str(art))
            if m and m.group(1).strip() == article_num:
                found_docs.append(f"{art}\n{doc.strip()}")

        if not found_docs:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c ƒëi·ªÅu lu·∫≠t.", f"ƒêi·ªÅu {article_num} (kh√¥ng th·∫•y trong DB)", "N/A"

        final_context = "\n---\n".join(found_docs)
        source_info = f"ƒêi·ªÅu {article_num} (t√¨m th·∫•y {len(found_docs)} ƒëo·∫°n)"

        # üö´ T·ª± ƒë·ªông b·ªè qua LLM n·∫øu c√¢u h·ªèi ch·ªâ d·∫°ng 'ƒêi·ªÅu X' ho·∫∑c t∆∞∆°ng t·ª±
        if re.fullmatch(r".*ƒêi·ªÅu\s*\d+.*", question.strip(), re.IGNORECASE):
            use_llm = False

            # N·∫øu c√¢u h·ªèi d√†i ho·∫∑c c√≥ th√™m n·ªôi dung (v√≠ d·ª•: "ƒêi·ªÅu 47 n√≥i g√¨ v·ªÅ...")
        # th√¨ v·∫´n cho ph√©p `use_llm` (n·∫øu user b·∫≠t)

    # === 2Ô∏è‚É£ Hybrid Search (BM25 + Embedding) (N·∫øu kh√¥ng ph·∫£i t√¨m theo 'ƒêi·ªÅu X') ===
    else:
        all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)
        documents = all_data.get("documents", [])
        metadatas = all_data.get("metadatas", [])
        if not documents:
            return "‚ö†Ô∏è CSDL tr·ªëng ho·∫∑c ch∆∞a t·∫£i ƒë√∫ng.", "", "N/A"

        tokenized_docs = [doc.lower().split() for doc in documents]
        bm25 = BM25Okapi(tokenized_docs)
        bm25_scores = bm25.get_scores(question.lower().split())
        top_bm25_idx = np.argsort(bm25_scores)[::-1][:TOP_K]
        bm25_docs = [(bm25_scores[i], documents[i], metadatas[i]) for i in top_bm25_idx if bm25_scores[i] > 0]

        sem_docs = retriever.invoke(question)
        merged, seen = [], set()

        for score, doc, meta in bm25_docs:
            art = meta.get("article", "Kh√¥ng r√µ") if isinstance(meta, dict) else "Kh√¥ng r√µ"
            if doc not in seen:
                merged.append((float(score), doc, art))
                seen.add(doc)

        q_vec = embed_query_vector(question, embed_tokenizer, embed_model)
        for d in sem_docs:
            d_vec = embed_query_vector(d.page_content, embed_tokenizer, embed_model)
            cos_sim = cosine_similarity(q_vec, d_vec)
            art = d.metadata.get("article", "Kh√¥ng r√µ")
            if d.page_content not in seen and cos_sim >= THRESHOLD:
                merged.append((float(cos_sim), d.page_content, art))
                seen.add(d.page_content)

        if not merged:
            return "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu lu·∫≠t li√™n quan.", "", "N/A"

        merged.sort(key=lambda x: x[0], reverse=True)
        best_score, _, best_art = merged[0]
        same_articles = [doc for score, doc, art in merged if art == best_art]
        final_context = f"{best_art}\n" + "\n".join(same_articles).strip()
        source_info = f"{best_art} (score={best_score:.2f})"

    # === 3Ô∏è‚É£ R√∫t g·ªçn Context v√† T√≠nh Metrics c∆° b·∫£n ===
    if len(final_context.split()) > 4000:
        final_context = " ".join(final_context.split()[:4000])

    try:
        # T√≠nh vector cho c√¢u h·ªèi (n·∫øu ch∆∞a c√≥) v√† ng·ªØ c·∫£nh
        if q_vec is None:
            q_vec = embed_query_vector(question, embed_tokenizer, embed_model)
        c_vec = embed_query_vector(final_context, embed_tokenizer, embed_model)

        # METRIC 1: Context Relevance
        context_relevance = cosine_similarity(q_vec, c_vec)
        metrics_str = f"üîπ Context Relevance (H·ªèi vs Ng·ªØ c·∫£nh): {context_relevance:.4f}\n"
    except Exception as e:
        metrics_str = f"L·ªói t√≠nh metric Context: {e}\n"
        c_vec = None  # ƒê·∫£m b·∫£o c_vec t·ªìn t·∫°i

    # === 4Ô∏è‚É£ X·ª≠ l√Ω n·∫øu KH√îNG g·ªçi LLM ===
    if not use_llm:
        metrics_str += "üîπ Groundedness (Tr·∫£ l·ªùi vs Ng·ªØ c·∫£nh): N/A (LLM Bypassed)\n"
        metrics_str += "üîπ Answer Relevance (Tr·∫£ l·ªùi vs H·ªèi): N/A (LLM Bypassed)"
        # Tr·∫£ v·ªÅ ch√≠nh ng·ªØ c·∫£nh l√†m c√¢u tr·∫£ l·ªùi
        return final_context, source_info, metrics_str

    # === 5Ô∏è‚É£ G·ªçi LLM n·∫øu c·∫ßn ===
    prompt_text = (
        prompt_template_quiz.format(context=final_context, question=question)
        if re.search(r"\b[a-e]\)", question.lower())
        else prompt_template_normal.format(context=final_context, question=question)
    )

    try:
        result = llm(prompt_text, max_new_tokens=512)
        answer = result[0]["generated_text"].strip()

        lines = [line.strip() for line in answer.splitlines() if line.strip()]
        unique_lines = remove_near_duplicates(lines, similarity=0.9)
        answer = "\n".join(unique_lines)

        # --- T√çNH METRICS CHO C√ÇU TR·∫¢ L·ªúI ---
        try:
            if c_vec is not None and q_vec is not None:
                a_vec = embed_query_vector(answer, embed_tokenizer, embed_model)

                # METRIC 2: Groundedness
                groundedness = cosine_similarity(a_vec, c_vec)
                metrics_str += f"üîπ Groundedness (Tr·∫£ l·ªùi vs Ng·ªØ c·∫£nh): {groundedness:.4f}\n"

                # METRIC 3: Answer Relevance
                answer_relevance = cosine_similarity(a_vec, q_vec)
                metrics_str += f"üîπ Answer Relevance (Tr·∫£ l·ªùi vs H·ªèi): {answer_relevance:.4f}"
            else:
                metrics_str += "Kh√¥ng th·ªÉ t√≠nh metrics (l·ªói vector c_vec/q_vec)"
        except Exception as e:
            metrics_str += f"L·ªói t√≠nh metric Tr·∫£ l·ªùi: {e}"
        # --- K·∫æT TH√öC T√çNH METRICS ---

    except Exception as e:
        answer = f"L·ªói khi sinh c√¢u tr·∫£ l·ªùi: {e}"
        metrics_str += "L·ªói sinh c√¢u tr·∫£ l·ªùi, kh√¥ng th·ªÉ t√≠nh metrics."

    return answer, source_info, metrics_str


# ======= Gradio UI =======
try:
    initialize_rag_components()
except Exception as e:
    print(f"‚ùå L·ªñI KH·ªûI T·∫†O NGHI√äM TR·ªåNG: {e}")

with gr.Blocks(title="‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam (Llama 1B LoRA)") as demo:
    gr.Markdown(f"""
    ## ‚öñÔ∏è Tr·ª£ l√Ω ph√°p l√Ω Lu·∫≠t D∆∞·ª£c Vi·ªát Nam
    **LLM:** `{LLM_MODEL}`  
    **Embedding:** `{EMBED_MODEL}`  
    ---
    """)

    with gr.Row():
        with gr.Column(scale=2):
            question = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi ph√°p l√Ω:", lines=3,
                                  placeholder="V√≠ d·ª•: ƒêi·ªÅu 47 quy ƒë·ªãnh g√¨ v·ªÅ thu·ªëc generic?")
            use_llm = gr.Checkbox(label="G·ªçi LLM (b·∫≠t ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi)", value=True)
            ask = gr.Button("H·ªèi", variant="primary")
            clear = gr.Button("Xo√°")
        with gr.Column(scale=3):
            answer_box = gr.Textbox(label="Tr·∫£ l·ªùi", lines=10, interactive=False)
            source_box = gr.Textbox(label="ƒêi·ªÅu lu·∫≠t tr√≠ch d·∫´n", lines=3, interactive=False)
            # TH√äM H·ªòP HI·ªÇN TH·ªä METRICS
            metrics_box = gr.Textbox(label="üìä Metrics ƒê√°nh gi√° (T∆∞∆°ng ƒë·ªìng Cosine)", lines=4, interactive=False)

    # C·∫¨P NH·∫¨T ƒê·∫¶U RA CHO N√öT "H·ªèi" V√Ä "Xo√°"
    ask.click(fn=rag_query, inputs=[question, use_llm], outputs=[answer_box, source_box, metrics_box])
    clear.click(lambda: ("", "", "", ""), outputs=[question, answer_box, source_box, metrics_box])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)