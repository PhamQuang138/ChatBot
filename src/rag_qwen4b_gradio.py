#!/usr/bin/env python3
import os
import re
import json
import torch
import numpy as np
import gradio as gr
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModel,
    BitsAndBytesConfig,
    pipeline
)
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFacePipeline
from langchain_core.embeddings import Embeddings
from langchain_core.prompts import ChatPromptTemplate


# ================== CONFIG ==================
BASE_DIR = "/home/quang/Documents/ChatBot"
CHROMA_PATH = os.path.join(BASE_DIR, "data", "chroma_db_qwen_embed_vn")
LLM_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
EMBED_MODEL = "Qwen/Qwen3-Embedding-0.6B"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TOP_K = 10
THRESHOLD = 0.01
BATCH_SIZE = 4
# ===========================================


# ===== Embedding wrapper =====
class Qwen3Embedding(Embeddings):
    def __init__(self, model, tokenizer, device="cpu", batch_size=4):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.batch_size = batch_size

    def embed_documents(self, texts):
        all_embs = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True,
                                    truncation=True, max_length=512).to(self.device)
            with torch.no_grad():
                outputs = self.model(**inputs)
                vecs = outputs.last_hidden_state[:, 0, :]
                vecs = vecs / (vecs.norm(dim=-1, keepdim=True) + 1e-12)
                all_embs.append(vecs.cpu().numpy())
        if not all_embs:
            return np.zeros((0, self.model.config.hidden_size)).tolist()
        return np.vstack(all_embs).tolist()

    def embed_query(self, text):
        return self.embed_documents([text])[0]


# ===== Utility =====
def cosine_similarity(a, b):
    a, b = np.asarray(a), np.asarray(b)
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    return float(np.dot(a, b) / denom) if denom != 0 else 0.0


def embed_query_vector(text: str, tokenizer, model):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)
        emb = outputs.last_hidden_state[:, 0, :]
        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-12)
    return emb.cpu().numpy()[0]


# ===== Initialization =====
vectordb = None
retriever = None
llm = None
prompt_template = None
embed_model = None
embed_tokenizer = None
embedding_fn = None

# ===== Initialization =====
def initialize_rag_components():
    global vectordb, retriever, llm, prompt_template, embed_model, embed_tokenizer, embedding_fn

    print("ðŸ› ï¸ Initializing RAG components...")

    # 1ï¸âƒ£ Load embedding model
    print(f"ðŸ”¹ Loading embedding model: {EMBED_MODEL}")
    embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)
    embed_model = AutoModel.from_pretrained(
        EMBED_MODEL,
        device_map="auto",
        torch_dtype=torch.float16
    )
    try:
        embed_model.to(DEVICE)
    except Exception:
        pass
    embedding_fn = Qwen3Embedding(embed_model, embed_tokenizer, DEVICE, BATCH_SIZE)
    print("âœ… Embedding model ready.")

    # 2ï¸âƒ£ Load Chroma DB
    if not os.path.exists(CHROMA_PATH):
        raise FileNotFoundError(f"âŒ Chroma DB not found at {CHROMA_PATH}")
    print(f"ðŸ”¹ Loading Chroma DB from {CHROMA_PATH}")
    vectordb = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_fn)
    retriever = vectordb.as_retriever(search_kwargs={"k": TOP_K})
    print("âœ… Chroma retriever ready.")

    # 3ï¸âƒ£ Load LLM (Qwen Instruct â€” dÃ¹ng pipeline trá»±c tiáº¿p)
    print(f"ðŸ”¹ Loading LLM {LLM_MODEL} (8-bit)...")
    bnb_config = BitsAndBytesConfig(load_in_8bit=True)
    tokenizer_llm = AutoTokenizer.from_pretrained(LLM_MODEL)
    model_llm = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        quantization_config=bnb_config
    )

    llm = pipeline(
        "text-generation",
        model=model_llm,
        tokenizer=tokenizer_llm,
        max_new_tokens=2048,
        max_length=4096,# cho phÃ©p sinh dÃ i hÆ¡n
        truncation=False,  # khÃ´ng cáº¯t context
        do_sample=False,
        return_full_text=False
    )

    print("âœ… LLM ready.")

    # 4ï¸âƒ£ Prompt Template (phiÃªn báº£n cá»±c nghiÃªm ngáº·t)
    prompt_template = ChatPromptTemplate.from_template(
        """Báº¡n lÃ  **trá»£ lÃ½ phÃ¡p lÃ½ chuyÃªn vá» Luáº­t DÆ°á»£c Viá»‡t Nam**.

    Dá»±a **chá»‰ trÃªn ná»™i dung Ä‘iá»u luáº­t trong CONTEXT** dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    Náº¿u **khÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p**, tráº£ lá»i Ä‘Ãºng cÃ¢u nÃ y:
    "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong cÃ¡c Ä‘iá»u luáº­t Ä‘Æ°á»£c cung cáº¥p."

    ---
    ### CONTEXT
    {context}

    ### CÃ‚U Há»ŽI
    {question}

    ### TRáº¢ Lá»œI (ngáº¯n gá»n, chÃ­nh xÃ¡c, trÃ­ch tá»« Ä‘iá»u luáº­t trÃªn,khÃ´ng láº·p nguyÃªn vÄƒn,
    TrÃ¬nh bÃ y Ä‘áº§y Ä‘á»§ táº¥t cáº£, giá»¯ nguyÃªn kÃ½ hiá»‡u cá»§a Ä‘iá»u luáº­t (vÃ­ dá»¥: a), b), c), d), Ä‘) ...), khÃ´ng Ä‘Æ°á»£c thay Ä‘á»•i,KhÃ´ng thÃªm pháº§n giáº£i thÃ­ch, khÃ´ng tÃ³m táº¯t, khÃ´ng bÃ¬nh luáº­n,
    Náº¿u cÃ¢u há»i chá»‰ há»i má»™t pháº§n (vÃ­ dá»¥ â€œhoáº¡t Ä‘á»™ng kinh doanh dÆ°á»£c gá»“m nhá»¯ng gÃ¬â€), váº«n giá»¯ nguyÃªn cáº¥u trÃºc Ä‘áº§y Ä‘á»§ cá»§a Ä‘iá»u luáº­t liÃªn quan.  ),


    """

    )

    print("âœ… All components initialized.\n")


from rank_bm25 import BM25Okapi



def rag_query(question: str, use_llm: bool = True):
    if not vectordb or not llm:
        return "âš ï¸ RAG chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o Ä‘Ãºng cÃ¡ch.", ""

    # === 1ï¸âƒ£ Xá»­ lÃ½ cÃ¢u há»i dáº¡ng "Äiá»u X" ===
    match = re.search(r"Äiá»u\s*(\d+)", question.strip(), re.IGNORECASE)
    if match:
        article_num = match.group(1).strip()
        all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)

        found_docs = []
        for doc, meta in zip(all_data.get("documents", []), all_data.get("metadatas", [])):
            art = meta.get("article", "") if isinstance(meta, dict) else ""
            m = re.search(r"(\d+)", str(art))
            if m and m.group(1).strip() == article_num:
                found_docs.append(f"{art}\n{doc.strip()}")

        if not found_docs:
            return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong cÃ¡c Ä‘iá»u luáº­t.", f"Äiá»u {article_num} (khÃ´ng tháº¥y trong DB)"

        context = "\n---\n".join(found_docs)
        # Náº¿u táº¯t LLM thÃ¬ chá»‰ in context
        if not use_llm:
            print(f"\nðŸ“š CONTEXT (Äiá»u {article_num}):\n", context[:2000], "\n====================\n")
            return context, f"Äiá»u {article_num} (tÃ¬m tháº¥y {len(found_docs)} Ä‘oáº¡n)"
        # NgÆ°á»£c láº¡i thÃ¬ tiáº¿p tá»¥c gá»i LLM
        question = f"Ná»™i dung cá»§a Äiá»u {article_num} lÃ  gÃ¬?"

    # === 2ï¸âƒ£ Hybrid Search: BM25 + Embedding ===
    all_data = vectordb._collection.get(include=["documents", "metadatas"], limit=10000)
    documents = all_data.get("documents", [])
    metadatas = all_data.get("metadatas", [])
    if not documents:
        return "âš ï¸ CSDL trá»‘ng hoáº·c chÆ°a táº£i Ä‘Ãºng.", ""

    tokenized_docs = [doc.lower().split() for doc in documents]
    bm25 = BM25Okapi(tokenized_docs)
    bm25_scores = bm25.get_scores(question.lower().split())
    top_bm25_idx = np.argsort(bm25_scores)[::-1][:TOP_K]
    bm25_docs = [(bm25_scores[i], documents[i], metadatas[i]) for i in top_bm25_idx if bm25_scores[i] > 0]

    sem_docs = retriever.invoke(question)

    merged, seen = [], set()
    for score, doc, meta in bm25_docs:
        art = meta.get("article", "KhÃ´ng rÃµ") if isinstance(meta, dict) else "KhÃ´ng rÃµ"
        if doc not in seen:
            merged.append((float(score), doc, art))
            seen.add(doc)

    q_vec = embed_query_vector(question, embed_tokenizer, embed_model)
    for d in sem_docs:
        d_vec = embed_query_vector(d.page_content, embed_tokenizer, embed_model)
        cos_sim = cosine_similarity(q_vec, d_vec)
        art = d.metadata.get("article", "KhÃ´ng rÃµ")
        if d.page_content not in seen and cos_sim >= THRESHOLD:
            merged.append((float(cos_sim), d.page_content, art))
            seen.add(d.page_content)

    if not merged:
        return "âš ï¸ KhÃ´ng tÃ¬m tháº¥y Ä‘iá»u luáº­t liÃªn quan.", ""

    merged.sort(key=lambda x: x[0], reverse=True)
    best_score, _, best_art = merged[0]
    same_articles = [doc for score, doc, art in merged if art == best_art]
    combined_content = "\n".join(same_articles)

    lines = [l.strip() for l in combined_content.splitlines() if l.strip()]
    unique_lines, seen_lines = [], set()
    for l in lines:
        if l not in seen_lines:
            unique_lines.append(l)
            seen_lines.add(l)
    cleaned_content = "\n".join(unique_lines)

    context = f"{best_art}\n{cleaned_content.strip()}"
    if len(context.split()) > 2000:
        context = " ".join(context.split()[:2000])
    sources = [f"{best_art} (score={best_score:.2f})"]

    if best_score < 5:
        return "KhÃ´ng tÃ¬m tháº¥y Ä‘iá»u luáº­t phÃ¹ há»£p.", ""

    # --- âš™ï¸ Náº¿u táº¯t LLM thÃ¬ chá»‰ hiá»ƒn thá»‹ context ---
    if not use_llm:
        print("\nðŸ§© CONTEXT TRUY XUáº¤T ÄÆ¯á»¢C:\n", context[:2000], "\n====================\n")
        return context, "\n".join(sources)

    # --- ðŸ§  Náº¿u báº­t LLM ---
    prompt = prompt_template.format(context=context, question=question)
    print("\nðŸ§© PROMPT Gá»¬I LÃŠN LLM:\n", prompt[:1000], "\n====================\n")

    try:
        result = llm(prompt, max_new_tokens=512)
        answer = result[0]["generated_text"].strip() if isinstance(result, list) else str(result).strip()
    except Exception as e:
        answer = f"Lá»—i khi sinh cÃ¢u tráº£ lá»i: {e}"

    # LÃ m sáº¡ch text
    answer = re.sub(r'(?i)\bassistant\s*[:ï¼š-]*\s*', '', answer)
    answer = re.sub(r'(?i)assistant\s+nÃ³i\s+ráº±ng[:ï¼š-]*\s*', '', answer)
    answer = re.sub(r'^[\s\n]+|[\s\n]+$', '', answer)
    answer = re.sub(r'\n{2,}', '\n', answer)
    answer = re.sub(r'([a-z]\))', r'\n\1', answer)
    answer = re.sub(r'(\d+\.)', r'\n\1', answer)
    answer = re.sub(r'(\n\s*)+', '\n', answer).strip()

    lines = [l.strip() for l in answer.splitlines() if l.strip()]
    deduped, seen = [], set()
    for l in lines:
        if l not in seen:
            deduped.append(l)
            seen.add(l)
    answer = "\n".join(deduped).strip()

    if not answer or "khÃ´ng tÃ¬m tháº¥y" in answer.lower():
        answer = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong cÃ¡c Ä‘iá»u luáº­t Ä‘Æ°á»£c cung cáº¥p."

    return answer, "\n".join(sources)

try:
    initialize_rag_components()
except Exception as e:
    print(f"âŒ Lá»–I KHá»žI Táº O NGHIÃŠM TRá»ŒNG: {e}")


# ======= Gradio UI =======
with gr.Blocks(title="âš–ï¸ Trá»£ lÃ½ phÃ¡p lÃ½ Luáº­t DÆ°á»£c Viá»‡t Nam (Qwen RAG)") as demo:
    gr.Markdown(f"""
    ## âš–ï¸ Trá»£ lÃ½ phÃ¡p lÃ½ Luáº­t DÆ°á»£c Viá»‡t Nam
    **LLM:** `{LLM_MODEL}`  
    **Embedding:** `{EMBED_MODEL}`  
    ---
    """)

    with gr.Row():
        with gr.Column(scale=2):
            question = gr.Textbox(label="Nháº­p cÃ¢u há»i phÃ¡p lÃ½:", lines=3,
                                  placeholder="VÃ­ dá»¥: Äiá»u 47 quy Ä‘á»‹nh gÃ¬ vá» thuá»‘c generic?")
            use_llm = gr.Checkbox(label="Gá»i LLM (báº­t Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i)", value=True)

            ask = gr.Button("Há»i", variant="primary")
            clear = gr.Button("XoÃ¡")
        with gr.Column(scale=3):
            answer_box = gr.Textbox(label="Tráº£ lá»i", lines=10, interactive=False)
            source_box = gr.Textbox(label="Äiá»u luáº­t trÃ­ch dáº«n", lines=6, interactive=False)

    ask.click(fn=rag_query, inputs=[question, use_llm], outputs=[answer_box, source_box])

    clear.click(lambda: ("", "", ""), outputs=[question, answer_box, source_box])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
